import re
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorWithPadding
import torch


# ----------------------------
# 1. æ•°æ®ç”Ÿæˆï¼šæ„é€  Soft-Masked æ•°æ®
def create_soft_masked_data(example):
    """
    é’ˆå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œé€å­—ç¬¦å¯¹æ¯” input å’Œ outputï¼š
      - å¦‚æœå­—ç¬¦ä¸ä¸€è‡´ï¼Œåˆ™è¾“å‡º â€œ[MASK]â€
      - å¦åˆ™è¾“å‡ºè¯¥å­—ç¬¦
    ç”Ÿæˆå­—æ®µ "masked_input"ã€‚
    """
    input_text = example["input"]
    output_text = example["output"]
    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œåˆ™ç›´æ¥è¿”å›åŸå¥
    if example["error_count"] == 0:
        return {"masked_input": input_text}

    tokens = []
    for a, b in zip(input_text, output_text):
        if a != b:
            tokens.append("[MASK]")
        else:
            tokens.append(a)
    # å¦‚æœ input æ¯” output é•¿ï¼ˆæˆ–åä¹‹ï¼‰ï¼Œè¿™é‡Œæš‚ç®€å•æŒ‰ zip ç»“æœå¤„ç†
    masked_input = "".join(tokens)
    return {"masked_input": masked_input}


# ----------------------------
# 2. åŠ è½½åŸå§‹æ•°æ®å¹¶ç”Ÿæˆ Soft-Masked æ•°æ®
data_files = {
    "train": "dataset/law_correction_train_split.jsonl",
    "validation": "dataset/law_correction_valid.jsonl",
    "test": "dataset/law_correction_test.jsonl"
}
raw_dataset = load_dataset("json", data_files=data_files)
print("åŸå§‹æ•°æ®é›† keys:", raw_dataset.keys())

# ç”Ÿæˆ soft-masked æ•°æ®ï¼ˆåªä¸º error_count>0 çš„æ ·æœ¬ä¼šæ”¹å˜ï¼Œå¦åˆ™ masked_input ä¸ input ç›¸åŒï¼‰
masked_dataset = raw_dataset.map(create_soft_masked_data)
print("ç¤ºä¾‹ soft-masked æ•°æ®:", masked_dataset["train"][0])

# ----------------------------
# 3. Tokenization åŠå¯¹é½ï¼šé€å­—ç¬¦å¤„ç†ï¼Œä¿è¯ â€œ[MASK]â€ ä¸è¢«æ‹†åˆ†
model_name = "hfl/chinese-macbert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)


def tokenize_for_softmasked(example):
    """
    å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼š
      - æ„é€  token åˆ—è¡¨ï¼šéå† input ä¸ outputï¼Œè‹¥å­—ç¬¦ä¸ä¸€è‡´ï¼Œè¾“å‡º â€œ[MASK]â€ï¼Œå¦åˆ™è¾“å‡ºåŸå­—ç¬¦ã€‚
      - ä½¿ç”¨ tokenizer(token_list, is_split_into_words=True) å¾—åˆ° input_ids ç­‰ã€‚
      - å¯¹é½æ ‡ç­¾ï¼šå¦‚æœå¯¹åº” token ä¸º â€œ[MASK]â€ï¼Œåˆ™ label ä¸º output ä¸­å¯¹åº”å­—ç¬¦è½¬æ¢çš„ token idï¼›å¦åˆ™ä¸º -100ã€‚
    """
    input_text = example["input"]
    output_text = example["output"]
    # æ„é€  token åˆ—è¡¨ï¼šå¦‚æœ error_count==0ï¼Œtoken_list = list(input_text)
    # å¦‚æœ error_count>0ï¼Œæ›¿æ¢ä¸åŒçš„å­—ç¬¦ä¸º "[MASK]"
    if example["error_count"] == 0:
        token_list = list(input_text)
    else:
        token_list = []
        for a, b in zip(input_text, output_text):
            if a != b:
                token_list.append("[MASK]")
            else:
                token_list.append(a)
    # tokenize
    tokenized = tokenizer(token_list, is_split_into_words=True, truncation=True, max_length=128, padding="max_length")

    # å¯¹é½æ ‡ç­¾
    labels = []
    # å¾—åˆ°æ¯ä¸ª token å¯¹åº”åŸå§‹ token_list çš„ç´¢å¼•
    word_ids = tokenized.word_ids()
    for idx in word_ids:
        if idx is None:
            labels.append(-100)
        else:
            # å¦‚æœ token_list[idx] æ˜¯ â€œ[MASK]â€ï¼Œåˆ™ label åº”è¯¥ä¸º output_text[idx] è½¬æ¢çš„ token id
            if token_list[idx] == "[MASK]":
                # å¯¹ output_text[idx] å•ç‹¬ tokenizeï¼Œåº”å½“è¿”å›å•ä¸ª token
                out_token = tokenizer.tokenize(output_text[idx])
                if out_token:
                    label_id = tokenizer.convert_tokens_to_ids(out_token)[0]
                else:
                    label_id = -100
                labels.append(label_id)
            else:
                labels.append(-100)
    tokenized["labels"] = labels
    return tokenized


# å¯¹ trainã€validationã€test åˆ†åˆ«å¤„ç†
tokenized_datasets = masked_dataset.map(tokenize_for_softmasked, batched=False)
print("é¢„å¤„ç†åç¤ºä¾‹ï¼š", tokenized_datasets["train"][0])

# ä½¿ç”¨ DataCollatorWithPadding æ¥åŠ¨æ€ padding
data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)

# ----------------------------
# 4. è®­ç»ƒ Soft-Masked MacBERT æ¨¡å‹
model = AutoModelForMaskedLM.from_pretrained(model_name)

training_args = TrainingArguments(
    output_dir="./macbert-soft-masked",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=50,
    save_strategy="epoch",
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()
eval_results = trainer.evaluate(tokenized_datasets["validation"])
print("è¯„ä¼°ç»“æœï¼š", eval_results)

# ----------------------------
# 5. æµ‹è¯• Soft-Masked çº é”™æ•ˆæœ
from transformers import pipeline

corrector = pipeline("fill-mask", model=model, tokenizer=tokenizer)

# ä»æµ‹è¯•é›†ä¸­é€‰æ‹©ä¸€ä¸ª error_count > 0 çš„æ ·æœ¬
sample = None
for ex in masked_dataset["test"]:
    if ex["error_count"] > 0:
        sample = ex
        break

if sample is None:
    print("æµ‹è¯•é›†ä¸­æ— é”™è¯¯æ ·æœ¬ï¼Œæ— æ³•æµ‹è¯•çº é”™æ•ˆæœã€‚")
else:
    print("åŸå§‹ input:", sample["input"])
    print("æ­£ç¡® output:", sample["output"])
    print("ç”Ÿæˆ masked_input:", sample["masked_input"])
    # ä½¿ç”¨ corrector å¯¹ masked_input è¿›è¡Œé¢„æµ‹
    results = corrector(sample["masked_input"])
    print("\nğŸ” çº é”™å€™é€‰ç»“æœï¼š")
    for res in results:
        print(f"å€™é€‰æ›¿æ¢: {res['token_str']} (ç½®ä¿¡åº¦: {res['score']:.4f})")
