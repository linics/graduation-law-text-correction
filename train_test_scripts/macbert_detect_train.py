from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, \
    DataCollatorForTokenClassification
from datasets import load_dataset

# 1. **åŠ è½½æ•°æ®**
data_files = {
    "train": "dataset/law_correction_train_split.jsonl",
    "validation": "dataset/law_correction_valid.jsonl",
    "test": "dataset/law_correction_test.jsonl"
}
dataset = load_dataset("json", data_files=data_files)

# 2. **åŠ è½½ Tokenizer å’Œ Model**
model_name = "hfl/chinese-macbert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)  # 2ç±»ï¼šæ­£ç¡®(0) / é”™è¯¯(1)


# 3. **é”™è¯¯ä½ç½®æ ‡æ³¨**
def find_error_positions(input_text, output_text):
    """
    è¯†åˆ« input å’Œ output ä¹‹é—´çš„ä¸åŒå­—ç¬¦ä½ç½®ï¼Œè¿”å›é”™è¯¯ä½ç½®åˆ—è¡¨
    """
    min_length = min(len(input_text), len(output_text))
    error_positions = [1 if i < min_length and input_text[i] != output_text[i] else 0 for i in range(len(input_text))]

    # å¦‚æœ output æ¯” input é•¿ï¼Œå¯èƒ½æ˜¯å°‘å­—é”™è¯¯
    if len(input_text) < len(output_text):
        error_positions.extend([1] * (len(output_text) - len(input_text)))

    return error_positions


# 4. **æ•°æ®é¢„å¤„ç†**
def tokenize_and_align_labels(examples):
    """
    å¤„ç†å¥å­çº§æ•°æ®ï¼š
    - Tokenize å¥å­
    - ç”Ÿæˆä¸ Tokenizer å¯¹é½çš„ Label
    """
    tokenized_inputs = tokenizer(examples["input"], padding="max_length", truncation=True, max_length=128)

    labels = []
    for i in range(len(examples["input"])):
        input_text = examples["input"][i]
        output_text = examples["output"][i]

        if examples["error_count"][i] == 0:
            # æ— é”™è¯¯ï¼Œå…¨éƒ¨æ ‡ 0
            label_ids = [0] * len(input_text)
        else:
            # è®¡ç®—é”™è¯¯ä½ç½®
            label_ids = find_error_positions(input_text, output_text)

        # ç¡®ä¿ä¸ tokenizer ç»“æœå¯¹é½
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        aligned_labels = []
        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                aligned_labels.append(-100)  # å¡«å……éƒ¨åˆ†ä¸è®¡ç®— loss
            elif word_idx != previous_word_idx:
                aligned_labels.append(label_ids[word_idx])  # ä»…å¯¹ç¬¬ä¸€ä¸ªå­è¯æ ‡æ³¨
            else:
                aligned_labels.append(-100)
            previous_word_idx = word_idx

        labels.append(aligned_labels)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


# 5. **å¤„ç†æ•°æ®**
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)
data_collator = DataCollatorForTokenClassification(tokenizer)

# 6. **è®­ç»ƒå‚æ•°**
training_args = TrainingArguments(
    output_dir="./macbert-error-detect",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_steps=50,
    save_strategy="epoch",
    save_total_limit=10,
)

# 7. **è®­ç»ƒ**
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

# 8. **æµ‹è¯•**
print("\nğŸ“Š å¼€å§‹æµ‹è¯•æ¨¡å‹...")
results = trainer.evaluate(tokenized_datasets["test"])
print(f"æµ‹è¯•é›†ç»“æœ: {results}")
